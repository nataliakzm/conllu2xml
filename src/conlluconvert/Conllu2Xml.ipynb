{"cells":[{"cell_type":"markdown","source":["### Upload"],"metadata":{"id":"eOPyJtOpICnn"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7574,"status":"ok","timestamp":1674638841696,"user":{"displayName":"NATALIA KUZMINYKH","userId":"04917015866921161966"},"user_tz":-60},"id":"RIFsd5GX1Lz0","outputId":"a8e2f45a-9f03-4899-ef9e-853b923a752d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n","  warn(RuntimeWarning(msg))\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["# Import the required modules\n","import argparse\n","from xml.dom import minidom\n","import html\n","import re\n","import os\n","\n","!python -m nltk.downloader punkt\n","from nltk.tokenize import sent_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"kaiivwiUAyNk","executionInfo":{"status":"ok","timestamp":1674641276264,"user_tz":-60,"elapsed":2434574,"user":{"displayName":"NATALIA KUZMINYKH","userId":"04917015866921161966"}},"outputId":"bd810bfe-f742-4216-a648-913c08071c3c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-1f1c7de4-5155-4a6e-bb1e-4c2b104e2b90\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1f1c7de4-5155-4a6e-bb1e-4c2b104e2b90\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving de_gsd-ud_CLEAN.conllu to de_gsd-ud_CLEAN.conllu\n","Saving de_hdt-ud_CLEAN.conllu to de_hdt-ud_CLEAN.conllu\n","Saving de_pud-ud_CLEAN.conllu to de_pud-ud_CLEAN.conllu\n"]}],"source":["# INPUT FILES\n","# Set the input directory\n","from google.colab import files\n","\n","# Create the Input folder\n","if not os.path.exists('/content/Input'):\n","    os.makedirs('/content/Input')\n","\n","# Create the Output folder\n","if not os.path.exists('/content/Output'):\n","    os.makedirs('/content/Output')\n","\n","# Upload the files\n","uploaded_files = files.upload()\n","\n","# Set the input directory\n","input_dir = '/content/Input'\n","\n","# Iterate over the uploaded files\n","for file_name, data in uploaded_files.items():\n","    # Construct the input file path\n","    input_path = os.path.join(input_dir, file_name)\n","    \n","    # Write the uploaded file to the input directory\n","    with open(input_path, 'wb') as f:\n","        f.write(data)"]},{"cell_type":"markdown","source":["## Conllu to XML"],"metadata":{"id":"9sYqmhz2ITgA"}},{"cell_type":"markdown","source":["### Let's see if I can replace the charaters\n"],"metadata":{"id":"QLEzwY-2FrCh"}},{"cell_type":"code","source":["# Set the input and output directories\n","args = argparse.Namespace()\n","args.input_dir = '/content/Input'\n","args.output_dir = '/content/Output'\n","\n","# Iterate over all the .conllu files in the input directory\n","for filename in os.listdir(args.input_dir):\n","    if not filename.endswith('.conllu'):\n","        continue\n","    \n","    # Construct the full file path\n","    file_path = os.path.join(args.input_dir, filename)\n","\n","    # Open the file in read mode\n","    with open(file_path, 'r') as f:\n","        conllu_string = f.read()\n","        \n","        # Split the conllu string into a list of lines\n","        conllu_lines = conllu_string.split('\\n')\n","\n","        # Create the xml document and root element\n","        xmldoc = minidom.Document()\n","        root = xmldoc.createElement('trf')\n","        xmldoc.appendChild(root)\n","\n","        # Iterate over the list of lines and extract the values from each line\n","        conllu_parsed = []\n","        sentence = []\n","        id = 1\n","\n","        for line in conllu_lines:\n","            if line.startswith('#'):\n","                continue\n","            elif line == '':\n","                conllu_parsed.append(sentence)\n","                sentence = []\n","            else:\n","                columns = line.split('\\t')\n","                form = columns[1]\n","                upos = columns[3]\n","                feats = columns[5]\n","                token = {'form': form, 'upos': upos, 'feats': feats}\n","                sentence.append(token)            \n","\n","        # Iterate over the list of sentences and create the xml elements\n","        for i, sentence in enumerate(conllu_parsed):\n","\n","          # Create the qitext element and set the id attribute\n","          qitext = xmldoc.createElement('qitext')\n","          qitext.setAttribute('id', str(id))\n","\n","          # Create the plain element and add it to the qitext element\n","          plain = xmldoc.createElement('plain')\n","          plain_text = ' '.join([token['form'] for token in sentence])\n","          plain.appendChild(xmldoc.createTextNode(plain_text))\n","          qitext.appendChild(plain)\n","\n","          # Set the l attribute of the qitext element to the length of the plain text\n","          qitext.setAttribute('l', str(len(plain_text)))\n","\n","          # Parse the plain text into a list of sentences using the nlp function\n","          sentences = sent_tokenize(plain_text, language='french')\n","\n","          # Iterate over the list of tokens and create the qitoken elements\n","          start = 0\n","          sentence_counter = 0    \n","\n","          for j, token in enumerate(sentence):\n","            qitoken = xmldoc.createElement('qitoken')\n","            qitoken_text = f\"{token['form']} {token['upos']}:{token['feats']}\"\n","            qitoken_text = re.sub(r'&(?!amp;|lt;|gt;)', '&amp;', qitoken_text)\n","            qitoken_text = re.sub(r'<http', '&lt;http', qitoken_text)\n","            qitoken_text = re.sub(r'<', '&lt;', qitoken_text)\n","            qitoken_text = re.sub(r'>', '&gt;', qitoken_text)\n","            qitoken_text = re.sub(r'<(?=[a-zA-Z])', '&lt;', qitoken_text)\n","            qitoken_text = re.sub(r'>(?=[a-zA-Z])', '&gt;', qitoken_text)\n","            qitoken.appendChild(xmldoc.createTextNode(qitoken_text))\n","            qitoken.setAttribute('start', str(start))\n","            start += len(token['form']) + 1\n","            qitoken.setAttribute('end', str(start - 1))\n","            qitoken.setAttribute('sentence', str(sentence_counter + 1))\n","            qitext.appendChild(qitoken)\n","            \n","            \n","            # Increment the sentence counter if the current token is the last in a sentence\n","            if sentence_counter < len(sentences) and token['form'] == sentences[sentence_counter][-1]:\n","              sentence_counter += 1\n","\n","          root.appendChild(qitext)\n","\n","          # Increment the id counter\n","          id += 1\n","\n","        # Construct the output file path\n","        output_filename = filename.replace('.conllu', '.xml')\n","        output_path = os.path.join(args.output_dir, output_filename)\n","    \n","        # Write the xml document to the output file\n","        with open(output_path, 'w') as f:\n","          f.write(html.unescape(xmldoc.toprettyxml()))\n","\n","        # Print the number of xml elements created\n","        print(f'{len(root.getElementsByTagName(\"qitext\"))} qitext elements created')"],"metadata":{"id":"XWcLLnaI6gXy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674554360643,"user_tz":-60,"elapsed":202549,"user":{"displayName":"NATALIA KUZMINYKH","userId":"04917015866921161966"}},"outputId":"1215d3c7-6164-4cbb-f658-62d7cc58a3df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["185380 qitext elements created\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"aswq3Vdw6fHT"}},{"cell_type":"markdown","metadata":{"id":"6CTUAhfNrNgI"},"source":["### This version (for simple cases)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249484,"status":"ok","timestamp":1674641525738,"user":{"displayName":"NATALIA KUZMINYKH","userId":"04917015866921161966"},"user_tz":-60},"id":"u1WxIhd51iLj","outputId":"113871ca-f10a-467d-fe6a-b2030350a3af"},"outputs":[{"output_type":"stream","name":"stdout","text":["999 qitext elements created\n","15559 qitext elements created\n","185380 qitext elements created\n"]}],"source":["# Set the input and output directories\n","args = argparse.Namespace()\n","args.input_dir = '/content/Input'\n","args.output_dir = '/content/Output'\n","\n","# Iterate over all the .conllu files in the input directory\n","for filename in os.listdir(args.input_dir):\n","    if not filename.endswith('.conllu'):\n","        continue\n","    \n","    # Construct the full file path\n","    file_path = os.path.join(args.input_dir, filename)\n","\n","    # Open the file in read mode\n","    with open(file_path, 'r') as f:\n","        conllu_string = f.read()\n","        \n","        # Split the conllu string into a list of lines\n","        conllu_lines = conllu_string.split('\\n')\n","\n","        # Create the xml document and root element\n","        xmldoc = minidom.Document()\n","        root = xmldoc.createElement('trf')\n","        xmldoc.appendChild(root)\n","\n","        # Iterate over the list of lines and extract the values from each line\n","        conllu_parsed = []\n","        sentence = []\n","        id = 1\n","\n","        for line in conllu_lines:\n","            if line.startswith('#'):\n","                continue\n","            elif line == '':\n","                conllu_parsed.append(sentence)\n","                sentence = []\n","            else:\n","                columns = line.split('\\t')\n","                form = columns[1]\n","                upos = columns[3]\n","                feats = columns[5]\n","                token = {'form': form, 'upos': upos, 'feats': feats}\n","                sentence.append(token)            \n","\n","        # Iterate over the list of sentences and create the xml elements\n","        for i, sentence in enumerate(conllu_parsed):\n","\n","          # Create the qitext element and set the id attribute\n","          qitext = xmldoc.createElement('qitext')\n","          qitext.setAttribute('id', str(id))\n","\n","          # Create the plain element and add it to the qitext element\n","          plain = xmldoc.createElement('plain')\n","          plain_text = ' '.join([token['form'] for token in sentence])\n","          plain.appendChild(xmldoc.createTextNode(plain_text))\n","          qitext.appendChild(plain)\n","\n","          # Set the l attribute of the qitext element to the length of the plain text\n","          qitext.setAttribute('l', str(len(plain_text)))\n","\n","          # Parse the plain text into a list of sentences using the nlp function\n","          sentences = sent_tokenize(plain_text, language='french')\n","\n","          # Iterate over the list of tokens and create the qitoken elements\n","          start = 0\n","          sentence_counter = 0\n","\n","          for j, token in enumerate(sentence):\n","            qitoken = xmldoc.createElement('qitoken')\n","            qitoken_text = f\"{token['form']} {token['upos']}:{token['feats']}\"\n","            qitoken.appendChild(xmldoc.createTextNode(qitoken_text))\n","            qitoken.setAttribute('start', str(start))\n","            start += len(token['form']) + 1\n","            qitoken.setAttribute('end', str(start - 1))\n","            qitoken.setAttribute('sentence', str(sentence_counter + 1))\n","            qitext.appendChild(qitoken)\n","            \n","            \n","            # Increment the sentence counter if the current token is the last in a sentence\n","            if sentence_counter < len(sentences) and token['form'] == sentences[sentence_counter][-1]:\n","              sentence_counter += 1\n","\n","          root.appendChild(qitext)\n","\n","          # Increment the id counter\n","          id += 1\n","\n","        # Construct the output file path\n","        output_filename = filename.replace('.conllu', '.xml')\n","        output_path = os.path.join(args.output_dir, output_filename)\n","    \n","        # Write the xml document to the output file\n","        with open(output_path, 'w') as f:\n","          f.write(html.unescape(xmldoc.toprettyxml()))\n","\n","        # Print the number of xml elements created\n","        print(f'{len(root.getElementsByTagName(\"qitext\"))} qitext elements created')\n"]},{"cell_type":"markdown","metadata":{"id":"uBbaa5EqzPOU"},"source":["### OLD version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2408,"status":"ok","timestamp":1673255473294,"user":{"displayName":"NATALIA KUZMINYKH","userId":"04917015866921161966"},"user_tz":-60},"id":"cXWGHiSaw-Iq","outputId":"6dfc1e73-95af-46a2-9494-bcac432affc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["999 qitext elements created\n","11800 qitext elements created\n"]}],"source":["from html import escape\n","import argparse\n","import os\n","from xml.dom import minidom\n","\n","# Set the input and output directories\n","args = argparse.Namespace()\n","args.input_dir = '/content/Input'\n","args.output_dir = '/content/Output'\n","\n","# Iterate over all the .conllu files in the input directory\n","for filename in os.listdir(args.input_dir):\n","    if not filename.endswith('.conllu'):\n","        continue\n","    \n","    # Construct the full file path\n","    file_path = os.path.join(args.input_dir, filename)\n","\n","    # Open the file in read mode\n","    with open(file_path, 'r') as f:\n","        conllu_string = f.read()\n","        \n","        # Split the conllu string into a list of lines\n","        conllu_lines = conllu_string.split('\\n')\n","\n","        # Create the xml document and root element\n","        xmldoc = minidom.Document()\n","        root = xmldoc.createElement('root')\n","\n","        # Add the XML declaration to the document\n","        #xmldoc.appendChild(xmldoc.createProcessingInstruction(\"xml\", \"version='1.0' encoding='UTF-8'\"))\n","\n","        # Append the root element to the xml document\n","        xmldoc.appendChild(root)\n","\n","        # Iterate over the list of lines and extract the values from each line\n","        conllu_parsed = []\n","        sentence = []\n","\n","        for line in conllu_lines:\n","            if line.startswith('#'):\n","                continue\n","            elif line == '':\n","                conllu_parsed.append(sentence)\n","                sentence = []\n","            else:\n","                columns = line.split('\\t')\n","                form = columns[1]\n","                upos = columns[3]\n","                feats = columns[5]\n","                token = {'form': form, 'upos': upos, 'feats': feats}\n","                sentence.append(token)\n","\n","        # Iterate over the list of sentences and create the xml elements\n","        for i, sentence in enumerate(conllu_parsed):\n","            # Create the qitext element and set the id attribute\n","            qitext = xmldoc.createElement('qitext')\n","\n","            # Create the plain element and add it to the qitext element\n","            plain = xmldoc.createElement('plain')\n","            plain_text = ' '.join([token['form'] for token in sentence])\n","            plain_text = escape(plain_text)  # Escape special characters in plain_text\n","            plain.appendChild(xmldoc.createTextNode(plain_text))\n","            qitext.appendChild(plain)\n","\n","            # Parse the plain text into a list of sentences using the nlp function\n","            sentences = sent_tokenize(plain_text, language='french')\n","\n","            for j, token in enumerate(sentence):\n","                qitoken = xmldoc.createElement('qitoken')\n","                qitoken_text = f\"{token['form']} {token['upos']}:{token['feats']}\"\n","                qitoken.appendChild(xmldoc.createTextNode(qitoken_text))\n","                qitext.appendChild(qitoken)\n","            root.appendChild(qitext)\n","\n","        # Construct the output file path\n","        output_filename = filename.replace('.conllu', '.xml')\n","        output_path = os.path.join(args.output_dir, output_filename)\n","    \n","        # Write the xml document to the output file\n","        with open(output_path, 'w') as f:\n","            f.write(html.unescape(xmldoc.toprettyxml()))\n","\n","        # Print the number of xml elements created\n","        print(f'{len(root.getElementsByTagName(\"qitext\"))} qitext elements created')\n"]},{"cell_type":"markdown","metadata":{"id":"QQVcD0lbA3Bk"},"source":["# DOWNLOAD"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1674642481252,"user":{"displayName":"NATALIA KUZMINYKH","userId":"04917015866921161966"},"user_tz":-60},"id":"U7l2PuFQA1SR","outputId":"17a454bc-5574-4f9a-88ca-971c25f89d3f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_a52cd9aa-b5bd-49d8-96a4-3bda3ca3a6d0\", \"de_hdt-ud_CLEAN.trf\", 325637449)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_2cf2f618-1f33-4ba0-9409-a3e1ac169611\", \"de_pud-ud_CLEAN.trf\", 2026222)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_a3612f53-c6cf-451b-8caa-21523e1fff20\", \"de_gsd-ud_CLEAN.trf\", 27456566)"]},"metadata":{}}],"source":["from google.colab import files\n","def download_xml_files():\n","    # Set the input and output directories\n","    input_dir = '/content/Output'\n","\n","    # Iterate over the .conllu files in the input directory\n","    for filename in os.listdir(input_dir):\n","        if not filename.endswith('.trf'):\n","            continue\n","\n","        # Construct the full file path\n","        file_path = os.path.join(input_dir, filename)\n","\n","        # Download the file\n","        files.download(file_path)\n","\n","download_xml_files()"]},{"cell_type":"markdown","metadata":{"id":"zJ4xgpvmkPHO"},"source":["# XML2TRF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfMOricuGUPf"},"outputs":[],"source":["#not correct\n","import os\n","\n","# Set the input and output file paths\n","input_file = '/content/Output/fr_pud-ud_DONE.xml'\n","output_file = '/content/fr_pud-ud.xml'#?TRF\n","\n","# Open the input file and read the contents into a variable\n","with open(input_file, 'r') as f:\n","    xml_data = f.read()\n","\n","# Replace the existing XML declaration with the new one\n","xml_data = xml_data.replace('<?xml version=\"1.0\" ?>', '<?xml version=\\'1.0\\' encoding=\\'UTF8\\' ?>\\n<format>3.0</format>')\n","\n","# Write the modified data to the output file\n","with open(output_file, 'w') as f:\n","    f.write(xml_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"executionInfo":{"elapsed":363,"status":"error","timestamp":1673259829044,"user":{"displayName":"NATALIA KUZMINYKH","userId":"04917015866921161966"},"user_tz":-60},"id":"rzOeqPYesBcl","outputId":"053a8523-ed1a-4c14-8d55-3dd707112f30"},"outputs":[{"ename":"OSError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-82fb29c514e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Open the XML file and ignore errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXMLParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree.parse\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._parseDocument\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._parseDocumentFromURL\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._parseDocFromFile\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._BaseParser._parseDocFromFile\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._ParserContext._handleParseResultDoc\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleParseResult\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._raiseParseError\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Error reading file 'French.xml': failed to load external entity \"French.xml\""]}],"source":["#supposed to work but gives errror\n","import codecs\n","from lxml import etree\n","\n","# Set the input and output directories\n","input_dir = '/content/Input'\n","output_dir = '/content/Outpu'\n","\n","# Iterate over all files in the input directory\n","for xml_file in os.listdir(input_dir):\n","  # Check if the file is an XML file\n","  if xml_file.endswith('.xml'):\n","    # Get the full file path\n","    xml_file_path = os.path.join(input_dir, xml_file)\n","\n","    # Get the TRF file name from the XML file name\n","    trf_file = os.path.splitext(xml_file)[0] + \".trf\"\n","\n","    # Open the XML file and ignore errors\n","    parser = etree.XMLParser(recover=True)\n","    tree = etree.parse(xml_file, parser=parser)\n","    root = tree.getroot()\n","\n","    # Open the TRF file\n","    trf = codecs.open(trf_file, \"w\", \"utf-8\")\n","\n","    # Write the TRF header\n","    trf.write('<?xml version=\\'1.0\\' encoding=\\'UTF8\\' ?>\\n')\n","    trf.write(\"<format>3.0</format>\\n\")\n","\n","    id_counter = 1\n","\n","    # Get the qitext elements\n","    qitext_elements = root.findall(\"./qitext\")\n","\n","    # Iterate over the qitext elements\n","    for qitext_element in qitext_elements:\n","      # Get the text for the qitext element\n","      text_element = qitext_element.find(\"plain\")\n","    if text_element is not None:\n","      text = text_element.text\n","    else:\n","      text = \"\"\n","    \n","    # Get the length of the text\n","    length = len(text) if text is not None else 0\n","    \n","    # Write the qitext element\n","    trf.write(\"<qitext id='{}' l='{}'>\\n\".format(id_counter, length))\n","    trf.write(\"<plain>{}</plain>\\n\".format(text))\n","    \n","    # Get the qitoken elements\n","    qitoken_elements = qitext_element.findall(\"qitoken\")\n","\n","    for token in qitoken_elements:\n","      start = token.attrib['start']\n","      end = token.attrib['end']\n","      sentence = token.attrib['sentence']\n","      trf.write(\"<qitoken start='{}' end='{}' sentence='{}'>{} WORD</qitoken>\\n\".format(start, end, sentence, token.text))\n","\n","    #for token in qitoken_elements:\n","      #trf.write(\"<qitoken>{} WORD</qitoken>\\n\".format(token.text))\n","\n","    # Write the end of the qitext element\n","    trf.write(\"</qitext>\\n\")\n","\n","    # Increment the ID counter\n","    id_counter += 1\n","\n","    # Close the TRF file\n","    trf.close()\n","    \n","    # Write the TRF file \n","    print(\"TRF file created: \" + trf_file)"]},{"cell_type":"markdown","metadata":{"id":"yKS854WbsBHu"},"source":["# This one"]},{"cell_type":"markdown","metadata":{"id":"71xc1ZJ1iVaf"},"source":["Issues:\n","* & not printed\n","* numbers"]},{"cell_type":"markdown","source":["#### & case"],"metadata":{"id":"eYq86I8mH374"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXxMtSgLGs-y"},"outputs":[],"source":["import os\n","import re\n","\n","input_directory = \"/content/Input\"\n","output_directory = \"/content/Output\"\n","\n","for filename in os.listdir(input_directory):\n","    if filename.endswith(\".xml\"):\n","        input_file_path = os.path.join(input_directory, filename)\n","        output_file_path = os.path.join(output_directory, filename)\n","\n","        with open(input_file_path, \"r\") as f:\n","            xml_str = f.read()\n","\n","        xml_str = re.sub(r'&(?!(amp|lt|gt);)', '&amp;', xml_str)\n","\n","        if not os.path.exists(output_directory):\n","            os.makedirs(output_directory)\n","        with open(output_file_path, \"w\") as f:\n","            f.write(xml_str)\n","\n","#with open(\"/content/Input/de_gsd-ud_CLEAN.xml\", \"r\") as f:\n","#    xml_str = f.read()\n","\n","#xml_str = re.sub(r'&(?!(amp|lt|gt);)', '&amp;', xml_str)\n","\n","#with open(\"modified_file.xml\", \"w\") as f:\n","#    f.write(xml_str)\n"]},{"cell_type":"markdown","source":["##To trf"],"metadata":{"id":"cGruyEi7H7Od"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19442,"status":"ok","timestamp":1674641656424,"user":{"displayName":"NATALIA KUZMINYKH","userId":"04917015866921161966"},"user_tz":-60},"id":"f-S9sjDxrJLb","outputId":"ddf14529-d778-4f9b-d75a-b8c96c0faea1"},"outputs":[{"output_type":"stream","name":"stdout","text":["TRF file created: /content/Output/de_hdt-ud_CLEAN.trf\n"]}],"source":["import os\n","import codecs\n","from lxml import etree\n","\n","# Get the XML file name from the directory\n","xml_file = '/content/Output/de_hdt-ud_CLEAN.xml'\n","\n","# Get the TRF file name from the XML file name\n","trf_file = os.path.splitext(xml_file)[0] + \".trf\"\n","\n","# Open the XML file and ignore errors\n","parser = etree.XMLParser(recover=True)\n","tree = etree.parse(xml_file, parser=parser)\n","root = tree.getroot()\n","\n","# Open the TRF file\n","trf = codecs.open(trf_file, \"w\", \"utf-8\")\n","\n","# Write the TRF header\n","trf.write('<?xml version=\\'1.0\\' encoding=\\'UTF8\\' ?>\\n')\n","trf.write(\"<format>3.0</format>\\n\")\n","\n","# Initialize the ID counter\n","id_counter = 1\n","\n","# Get the qitext elements\n","qitext_elements = root.findall(\"./qitext\")\n","\n","# Iterate over the qitext elements\n","for qitext_element in qitext_elements:\n","    # Get the text for the qitext element\n","    text_element = qitext_element.find(\"plain\")\n","    if text_element is not None:\n","        text = text_element.text\n","    else:\n","        text = \"\"\n","    \n","    # Get the length of the text\n","    length = len(text) if text is not None else 0\n","    \n","    # Write the qitext element\n","    trf.write(\"<qitext id='{}' l='{}'>\\n\".format(id_counter, length))\n","    trf.write(\"<plain>{}</plain>\\n\".format(text))\n","    \n","    # Get the qitoken elements\n","    qitoken_elements = qitext_element.findall(\"qitoken\")\n","\n","    for token in qitoken_elements:\n","      start = token.attrib['start']\n","      end = token.attrib['end']\n","      #sentence = '1' # added the sentence variable\n","      trf.write(\"<qitoken start='{}' end='{}' sentence='1'>{} WORD OTHER</qitoken>\\n\".format(start, end, token.text))\n","      #trf.write(\"<qitoken start='{}' end='{}'>{} WORD</qitoken>\\n\".format(start, end, token.text))\n","\n","    # Write the end of the qitext element\n","    trf.write(\"</qitext>\\n\")\n","\n","    # Increment the ID counter\n","    id_counter += 1\n","\n","# Close the TRF file\n","trf.close()\n","\n","# Write the TRF file \n","print(\"TRF file created: \" + trf_file)"]},{"cell_type":"markdown","metadata":{"id":"SJj90MRRLGmy"},"source":["#Let's parse it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPjTyQnfLJLV"},"outputs":[],"source":["import os\n","import re\n","\n","# Specify the folder containing the TRF files\n","folder = \"/content/Output\"\n","\n","# Use the os.listdir() function to find all TRF files in the specified folder\n","for filename in os.listdir(folder):\n","    if filename.endswith(\".trf\"):\n","        filepath = os.path.join(folder, filename)\n","\n","        # Open the TRF file\n","        with open(filepath, \"r\") as f:\n","            trf_file = f.read()\n","\n","        # Use regular expressions to find all instances of \"qitoken\" tags and remove \":_\" from the 2nd argument\n","        trf_file = re.sub(r'qitoken(.*?) (\\w+):_', r'qitoken\\1 \\2', trf_file)\n","\n","        # Save the modified TRF file\n","        with open(filepath, \"w\") as f:\n","            f.write(trf_file)"]},{"cell_type":"markdown","source":["# REDUCE THE SIZE\n"],"metadata":{"id":"MA5rBmjHezRI"}},{"cell_type":"code","source":["def delete_lines(file_path, start_id):\n","    new_lines = []\n","    with open(file_path, \"r\") as file:\n","        lines = file.readlines()\n","        for line in lines:\n","            if \"qitext id\" in line and int(line.split(\"'\")[1]) < start_id:\n","                new_lines.append(line)\n","            elif \"qitext id\" in line and int(line.split(\"'\")[1]) >= start_id:\n","                break\n","            else:\n","                new_lines.append(line)\n","\n","    with open(file_path, \"w\") as file:\n","        file.writelines(new_lines)\n","\n","delete_lines(\"/content/es_ancora-ud.trf\",10001)"],"metadata":{"id":"YvfHVm4pjRxK","executionInfo":{"status":"ok","timestamp":1675088217714,"user_tz":-60,"elapsed":605,"user":{"displayName":"NATALIA KUZMINYKH","userId":"04917015866921161966"}}},"execution_count":15,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["uBbaa5EqzPOU","zJ4xgpvmkPHO"],"provenance":[],"authorship_tag":"ABX9TyPbGKSxz51biIwwcGqsV8l9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}